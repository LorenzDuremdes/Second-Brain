1. RoBERTa; modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates^[https://arxiv.org/abs/1907.11692]
	1. training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results

# related
1. [[BERT (language model)]]