1. principle of maximum [[entropy]]; states that the [[probability distribution]] which best represents the current state of [[knowledge]] about a [[system]] is the one with largest [[entropy]], in the context of precisely stated prior data (such as a proposition that expresses testable [[information]])^[https://en.wikipedia.org/wiki/Principle_of_maximum_entropy]
	1. since the distribution with the maximum [[entropy]] is the one that makes the fewest assumptions about the true distribution of data, the principle of maximum [[entropy]] can be seen as an application of [[Occam's razor]]

# extensional definition
1. There is a sequence such as 2, 4, 6, 8, and the test subject needs to predict the next number. Of course the pattern is immediately clear: The numbers are increasing by 2 each time, or more mathematically, the kth item is given by 2k. An intelligent person would easily identify this pattern and predict the next digit to be 10. However, the polynomial 2k4 − 20k3 + 70k2 − 98k + 48 is also consistent with the data, in which case the next number in the sequence would be 58. Why then, even if we are aware of the larger polynomial, do we consider the first answer to be the most likely one?
	1. principle of maximum [[entropy]] / [[Occam's razor]]^[Universal Intelligence - A Definition of Machine Intelligence, p. 20]
	2. **related**
		1. [[adaptive replacement cache]]