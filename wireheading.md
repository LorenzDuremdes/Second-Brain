# [[artificial intelligence]]
1. wireheading; a scenario where an intelligent agent (such as an AI system) modifies its own reward function or feedback signal to achieve its objectives in a way that is unintended or harmful to its human operators^[GPT-4]^[Superintelligence Paths, Dangers, Strategies (Nick Bostrom), p. ~220]

## resolve
1. AIXI → wireheading
	1. the former might resolve wireheading e.g. preventing the agent from just relying on immediate rewards^[GPT-4]^[Superintelligence Paths, Dangers, Strategies (Nick Bostrom), p. ~230]