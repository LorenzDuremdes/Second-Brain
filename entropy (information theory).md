1. [[entropy]] ([[information]] theory); of a random variable is the average level of "[[information]]", "surprise", or "uncertainty" inherent in the variable's possible outcomes
	1. uniform probability yields maximum uncertainty and therefore maximum entropy e.g. fair coin tossing
	2. e.g. if one side of a coin weighed more, it would provide more certainty and, therefore, lower entropy
2. entropy ([[information]] theory) â†’ dimension
	1. 5th dimension (usually)