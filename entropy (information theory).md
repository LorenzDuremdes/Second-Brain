1. [[entropy]] ([[information]] theory); of a random variable is the average level of "[[information]]", "surprise", or "uncertainty" inherent in the variable's possible outcomes
	1. uniform [[probability]] yields maximum uncertainty and therefore maximum [[entropy]] e.g. fair coin tossing
	2. e.g. if one side of a coin weighed more, it would provide more certainty and, therefore, lower [[entropy]]
2. [[entropy]] ([[information]] theory) → dimension
	1. 5th dimension (usually)
3. [[construal level theory]] → [[entropy]] (information theory)
	1. positive}} correlation ([[logical disjunction]])
	2. negative correlation ([[logical conjunction]])